{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sentiment_analysis2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyO1RF2U0TuH4Zzo+8bd1wSt"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"EHx-NApkp7T9","colab_type":"text"},"source":["# Setup"]},{"cell_type":"code","metadata":{"id":"Y19YAb1ip4-n","colab_type":"code","outputId":"0701cdf4-e7bf-4da5-9124-5762796a3f01","executionInfo":{"status":"ok","timestamp":1579629384866,"user_tz":360,"elapsed":6141,"user":{"displayName":"Michael Choi","photoUrl":"","userId":"15852591795367329590"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sat Jan 18 17:36:38 2020\n","@author: TiwarisUSA\n","\n","* mchoi converted to ipynb file to run in colab\n","\"\"\"\n","!pip install twitter\n","!pip install pyprind\n","\n","#mount local Google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","import twitter\n","from datetime import datetime\n","import time\n","import re\n","import sys\n","import pandas as pd\n","import pyprind as pp\n","import argparse\n","\n","#Update path to import from \"Finance-NLP-master\" directory in My Drive\n","sys.path.append('/content/gdrive/My Drive/Finance-NLP-master')\n","import oauth_info as auth # our local file with the OAuth infos\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: twitter in /usr/local/lib/python3.6/dist-packages (1.18.0)\n","Requirement already satisfied: pyprind in /usr/local/lib/python3.6/dist-packages (2.11.2)\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FAYznR98qDOW","colab_type":"text"},"source":["#TimelineMiner Class definition"]},{"cell_type":"code","metadata":{"id":"0r48W_FaqDee","colab_type":"code","colab":{}},"source":["class TimelineMiner(object):\n","  def __init__(self, access_token, access_secret, consumer_key, consumer_secret, user_name):\n","    self.access_token = auth.ACCESS_TOKEN\n","    self.access_secret = auth.ACCESS_TOKEN_SECRET\n","    self.consumer_key = auth.CONSUMER_KEY\n","    self.consumer_secret = auth.CONSUMER_SECRET\n","    self.user_name = auth.USER_NAME\n","    self.auth = None\n","    self.df = pd.DataFrame(columns=['timestamp', 'tweet'], dtype='str')\n","  \n","\n","  def authenticate(self):\n","    self.auth = twitter.Twitter(auth=twitter.OAuth(self.access_token, \n","                self.access_secret, self.consumer_key, \n","                self.consumer_secret))\n","    return bool(isinstance(self.auth, twitter.api.Twitter))\n","        \n","\n","  def get_timeline(self, max=0):\n","      keywords = ['faang', 'apple', 'facebook', 'netflix', 'google', 'amazon']\n","      #self.df['keywords'] = 'need'\n","      tweet_ids = [self.auth.statuses.user_timeline(screen_name = self.user_name, count=10)[0]['id']] # the ID of my last tweet\n","      last_count = 200\n","      counter = 0\n","      while last_count == 200:\n","        try:\n","          timeline = self.auth.statuses.user_timeline(screen_name = self.user_name, count=200, max_id=tweet_ids[-1])\n","          for tweet in range(len(timeline)):\n","            try:\n","              text = timeline[tweet]['text'].replace('\"', '\\'')\n","              tweet_id = int(timeline[tweet]['id'])\n","              date = self.__get_date(timeline, tweet)\n","                        \n","              if keywords:\n","                for k in keywords:\n","                  if self.__check_keyword(text,k):\n","                    self.df.loc[counter,'tweet'] = text\n","                    self.df.loc[counter,'timestamp'] = date\n","                    try:\n","                      self.df.loc[counter,'keywords'].append(k)\n","                    except AttributeError:\n","                      self.df.loc[counter,'keywords'] = [k]\n","                    try:\n","                      self.df.loc[counter,'keywords'] = ';'.join(self.df.loc[counter,'keywords'])\n","                    except KeyError:\n","                      pass\n","                  else:\n","                    self.df.loc[counter,'tweet'] = text\n","                    self.df.loc[counter,'timestamp'] = date\n","                  counter += 1\n","                        \n","                  if max and counter >= max:\n","                    break\n","                  sys.stdout.flush()   \n","                  sys.stdout.write('\\rTweets downloaded: %s' %counter)   \n","            except:\n","              print('Exception_inner')\n","        except:\n","          print('Exception_outer') \n","        if max and counter >= max:              \n","          break\n","      last_count = len(timeline)            \n","      tweet_ids.append(timeline[-1]['id'])           \n","      time.sleep(1)\n","      self.df.to_csv('RitholtzWealth.csv')      \n","        \n","  def make_csv(self, path):\n","    self.df.to_csv(path, encoding='utf8')\n","\n","  def __get_date(self, timeline, tweet):\n","    timest = datetime.strptime(timeline[tweet]['created_at'],\n","                               \"%a %b %d %H:%M:%S +0000 %Y\")\n","    date = timest.strftime(\"%Y-%d-%m %H:%M:%S\")\n","    return date\n","    \n","  def __check_keyword(self, s, key):\n","    return bool(re.search(key, s, re.IGNORECASE))\n","        \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QskiOMLWqYtB","colab_type":"text"},"source":["#MAIN Code"]},{"cell_type":"code","metadata":{"id":"GmhMvlYXp5Iu","colab_type":"code","colab":{}},"source":["parser = argparse.ArgumentParser(\n","        description='A command line tool to download your personal twitter timeline.',\n","        formatter_class=argparse.RawTextHelpFormatter,\n","        epilog='\\nExample:\\n./twitter_timeline.py -o my_timeline.csv -k Python,Github')\n","\n","parser.add_argument('-o', '--out', help='Filename for creating the output CSV file.')\n","parser.add_argument('-m', '--max', help='Maximum number (integer) of timeline tweets query (searches all by default)')\n","parser.add_argument('-k', '--keywords', help='A comma separated list of keywords for filtering (optional).')\n","parser.add_argument('-v', '--version', action='version', version='v. 1.0.1')\n","args = parser.parse_args()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1MHvvKMxQSi0","colab_type":"code","colab":{}},"source":["\n","\n","if not args.out:\n","  print('Please provide a filename for creating the output CSV file.')\n","  quit()\n","\n","tm = TimelineMiner(auth.ACCESS_TOKEN, \n","                   auth.ACCESS_TOKEN_SECRET,  \n","                   auth.CONSUMER_KEY, \n","                   auth.CONSUMER_SECRET,\n","                   auth.USER_NAME)\n","                   \n","if not args.max:\n","  max_t = 0\n","else:\n","  max_t = int(args.max)\n","  \n","if args.keywords:\n","  keywords = args.keywords.split(',')\n","else:\n","  keywords = args.keywords\n","    \n","print('Authentification successful: %s' %tm.authenticate())\n","tm.get_timeline(max=max_t)"],"execution_count":0,"outputs":[]}]}